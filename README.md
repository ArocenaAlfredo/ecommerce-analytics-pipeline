# E-commerce Analytics Pipeline

This project implements an end-to-end ELT (Extract, Load, Transform) data pipeline to analyze e-commerce transactions and delivery performance in Brazil between 2016 and 2018. The goal is to provide actionable insights on revenue generation and logistics efficiency for a major e-commerce platform in Latam.

## ğŸš€ Project Overview

The business team requested the Data Science department to analyze two key aspects:

- **Revenue**: total income per year, top and bottom product categories, revenue by state.
- **Delivery**: delays, differences between estimated vs. actual delivery dates, correlation with public holidays.

This project integrates real-world datasets from multiple sources, processes them using Python and SQL, stores them in SQLite, and visualizes results with Python libraries.

## ğŸ“Š Data Sources

- **E-commerce dataset**: Public dataset from Olist Store with over 100k anonymized orders made in Brazil (2016â€“2018).
- **Public Holidays API**: [`https://date.nager.at`](https://date.nager.at) â€” used to analyze delivery delays during holidays.

## ğŸ› ï¸ Technologies Used

- **Python**: Main programming language
- **Pandas**: Data manipulation from CSVs
- **Requests**: API consumption
- **SQLite**: Local database engine
- **SQL**: Data loading and querying
- **Matplotlib / Seaborn**: Data visualization
- **Jupyter Notebooks**: Interactive reporting
- **Black**: Code formatting
- **Pytest**: Unit testing

##  Project Structure
.
â”œâ”€â”€ dataset/ # CSV files from Olist
â”œâ”€â”€ notebooks/ # Jupyter notebooks with analysis
â”œâ”€â”€ src/ # Python scripts for ETL pipeline
â”œâ”€â”€ tests/ # Unit tests
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ images/ # ER diagram and figures

bash
Copiar
Editar

##  Installation

It is recommended to use a virtual environment:

```bash
python -m venv venv
source venv/bin/activate  # or venv\Scripts\activate on Windows
pip install -r requirements.txt
ğŸ§ª Running Tests
To verify the integrity of the code, run:

bash
Copiar
Editar
pytest tests/
Running the Pipeline
Place the unzipped dataset in the dataset/ directory.

Launch the main notebook or run the scripts in src/ to extract, load, and transform the data.

Visualizations and insights are available in the notebooks/ section.

Code Style
This project uses Black for code formatting:

bash
Copiar
Editar
black --line-length=88 .
ğŸ“ˆ Sample Insights
Revenue trends by year and state

Top/bottom product categories

Delivery delay patterns and holiday impact
